{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Web Scraping For Beginners with Python](https://medium.com/@durgaswaroop/web-scraping-with-python-introduction-7b3c0bbb6053)\n",
    "* [Web Scraping in Python](https://medium.com/dreidev/web-scraping-in-python-e07fba0a1663)\n",
    "* [Web Scraping](https://medium.com/tag/web-scraping)\n",
    "* [Web Scraping Tutorial with Python: Tips and Tricks](https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071)\n",
    "* [SQLite Python tutorial](http://zetcode.com/db/sqlitepythontutorial/)\n",
    "\n",
    "# Part 1: [Web Scraping For Beginners with Python](https://medium.com/@durgaswaroop/web-scraping-with-python-introduction-7b3c0bbb6053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installation of beautifulsoup \n",
    "#!pip list | grep beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request as ureq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "freblogg_url = 'http://freblogg.com'\n",
    "website = ureq.urlopen(freblogg_url).read()\n",
    "#print(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are calling the `urlopen()` method and passing our website url to it. The return value would be an object of class `http.client.HTTPResponse`. By calling `read()` on that object, we get all of the html for that site.\n",
    "\n",
    "This will print all of the html of the website and you should be able to see all the html of the homepage of the site you’re scraping.\n",
    "\n",
    "Technically, at this point you are done as you have the html of the website. But until you use this html to get some information, it is useless. So, let’s do that.\n",
    "\n",
    "Say, I want to extract titles of all the posts on my website’s homepage. At the time of writing this post, the first three articles are:\n",
    "\n",
    "* `How to recover from ‘git reset — hard” | Git`\n",
    "* `Functions in C Programming | Part 1`\n",
    "* `Matrix Multiplication | C Programming`\n",
    "\n",
    "You should be able to see these as well if you visit Freblogg now, below this article. So, my goal is to extract this information from the html we just got. We will use Beautifulsoup for this very task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Beautifulsoup\n",
    "To tell beautifulsoup to read our html, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs(website, \"html.parser\")\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`website` is the html we scraped from the site. We are passing to beautifulsoup along with an argument `html.parser`. This is our way of telling that we’re interested in using the default html parser. If we have our own custom parsers (which we don’t), we can use that here. Not giving this argument also works. i.e., `soup = bs(webpage)` would also work, as `html.parser` is the default parser. But it never hurts to be more specific.\n",
    "\n",
    "Now that we have our `soup` object, we can use that to get what we want. In my case, I need to get the titles of all the articles on my homepage.\n",
    "\n",
    "To do this we have to take a look at the website you are scraping and see what identifies the things we want to extract.\n",
    "\n",
    "In my case, all the titles of the articles are all `h2` headers as evident from the following html source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h2 class=\"descriptionheader\"&gt;\n",
       "&lt;/h2&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 12, 2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 10, 2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  h2\n",
       "0               <h2 class=\"descriptionheader\">\n",
       "</h2>\n",
       "1  <h2 class=\"date-header\"><span>January 12, 2018...\n",
       "2  <h2 class=\"post-title entry-title\"><span class...\n",
       "3  <h2 class=\"date-header\"><span>January 10, 2018...\n",
       "4  <h2 class=\"post-title entry-title\"><span class..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = soup.find_all('h2')\n",
    "df_data = pd.DataFrame({'h2':h2})\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us a list of all the `<h2>` tags in the page.\n",
    "\n",
    "Here we’re searching for all of the `<h2>` tags. Similarly if we want to get all `<div>` tags, we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h2</th>\n",
       "      <th>div</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h2 class=\"descriptionheader\"&gt;\n",
       "&lt;/h2&gt;</td>\n",
       "      <td>&lt;div class=\"main-container\"&gt;\n",
       "&lt;!-- begin header...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 12, 2018...</td>\n",
       "      <td>&lt;div class=\"header_container_fixed\"&gt;\n",
       "&lt;!-- begi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "      <td>&lt;div class=\"headersec section\" id=\"headersec\"&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 10, 2018...</td>\n",
       "      <td>&lt;div class=\"widget Header\" data-version=\"1\" id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "      <td>&lt;div id=\"header-inner\"&gt;\n",
       "&lt;div class=\"titlewrapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  h2  \\\n",
       "0               <h2 class=\"descriptionheader\">\n",
       "</h2>   \n",
       "1  <h2 class=\"date-header\"><span>January 12, 2018...   \n",
       "2  <h2 class=\"post-title entry-title\"><span class...   \n",
       "3  <h2 class=\"date-header\"><span>January 10, 2018...   \n",
       "4  <h2 class=\"post-title entry-title\"><span class...   \n",
       "\n",
       "                                                 div  \n",
       "0  <div class=\"main-container\">\n",
       "<!-- begin header...  \n",
       "1  <div class=\"header_container_fixed\">\n",
       "<!-- begi...  \n",
       "2  <div class=\"headersec section\" id=\"headersec\">...  \n",
       "3  <div class=\"widget Header\" data-version=\"1\" id...  \n",
       "4  <div id=\"header-inner\">\n",
       "<div class=\"titlewrapp...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div = soup.find_all('div')\n",
    "df_data['div'] = pd.Series(div)\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "<h2 class=\"post-title entry-title\"><span class=\"post_title_icon\"></span>\n",
      "<a href=\"http://www.freblogg.com/2018/01/webapp-with-flask-1.html\">Build A Web Application With Flask In Python Part I</a>\n",
      "</h2>\n",
      "-------------------------------------\n",
      "<h2 class=\"post-title entry-title\"><span class=\"post_title_icon\"></span>\n",
      "<a href=\"http://www.freblogg.com/2018/01/json-parsing-with-python.html\">Json Parsing With Python</a>\n",
      "</h2>\n",
      "-------------------------------------\n",
      "<h2 class=\"post-title entry-title\"><span class=\"post_title_icon\"></span>\n",
      "<a href=\"http://www.freblogg.com/2018/01/apache-spark-datasets-3.html\">Datasets In Apache Spark - Part 3 | Writing Datasets to Disk</a>\n",
      "</h2>\n"
     ]
    }
   ],
   "source": [
    "for i in headers[:3]:\n",
    "    print('-------------------------------------')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken out some of the output to keep it short, but as you can see, the headers has more things in it than just the article titles. The actual one’s I need are the `<h2>` tags with `class=\"post-title entry-title\"`. The other tags in the output like `<h2>★ Labels</h2>` or `<h2>★ Trending</h2>` are things that we don’t want.\n",
    "\n",
    "From this output we figured out that the it is `class=\"post-title entry-title\"` that actually defines the article title along with the `<h2>` tag. So, we will use that by adding an attribute dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h2</th>\n",
       "      <th>div</th>\n",
       "      <th>headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h2 class=\"descriptionheader\"&gt;\n",
       "&lt;/h2&gt;</td>\n",
       "      <td>&lt;div class=\"main-container\"&gt;\n",
       "&lt;!-- begin header...</td>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 12, 2018...</td>\n",
       "      <td>&lt;div class=\"header_container_fixed\"&gt;\n",
       "&lt;!-- begi...</td>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "      <td>&lt;div class=\"headersec section\" id=\"headersec\"&gt;...</td>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;h2 class=\"date-header\"&gt;&lt;span&gt;January 10, 2018...</td>\n",
       "      <td>&lt;div class=\"widget Header\" data-version=\"1\" id...</td>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "      <td>&lt;div id=\"header-inner\"&gt;\n",
       "&lt;div class=\"titlewrapp...</td>\n",
       "      <td>&lt;h2 class=\"post-title entry-title\"&gt;&lt;span class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  h2  \\\n",
       "0               <h2 class=\"descriptionheader\">\n",
       "</h2>   \n",
       "1  <h2 class=\"date-header\"><span>January 12, 2018...   \n",
       "2  <h2 class=\"post-title entry-title\"><span class...   \n",
       "3  <h2 class=\"date-header\"><span>January 10, 2018...   \n",
       "4  <h2 class=\"post-title entry-title\"><span class...   \n",
       "\n",
       "                                                 div  \\\n",
       "0  <div class=\"main-container\">\n",
       "<!-- begin header...   \n",
       "1  <div class=\"header_container_fixed\">\n",
       "<!-- begi...   \n",
       "2  <div class=\"headersec section\" id=\"headersec\">...   \n",
       "3  <div class=\"widget Header\" data-version=\"1\" id...   \n",
       "4  <div id=\"header-inner\">\n",
       "<div class=\"titlewrapp...   \n",
       "\n",
       "                                             headers  \n",
       "0  <h2 class=\"post-title entry-title\"><span class...  \n",
       "1  <h2 class=\"post-title entry-title\"><span class...  \n",
       "2  <h2 class=\"post-title entry-title\"><span class...  \n",
       "3  <h2 class=\"post-title entry-title\"><span class...  \n",
       "4  <h2 class=\"post-title entry-title\"><span class...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = soup.find_all('h2', attrs = {'class':'post-title entry-title'})\n",
    "df_data['headers'] = pd.Series(headers)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is just the article headers. Better than what we had before. From here we just have one more thing to do before we actually get the title.\n",
    "\n",
    "Before we get the article titles, let me show you a couple of cases of parsing with `beautifulsoup`. Till now we’re using `find_all` to get all the tags we want. Instead let’s use `find` which gives just one element instead of a list of all the headers.\n",
    "\n",
    "Using the python REPL, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"post-title entry-title\"><span class=\"post_title_icon\"></span>\n",
       "<a href=\"http://www.freblogg.com/2018/01/webapp-with-flask-1.html\">Build A Web Application With Flask In Python Part I</a>\n",
       "</h2>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2', attrs = {'class':'post-title entry-title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"post_title_icon\"></span>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2', {'class':'post-title entry-title'}).find('span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method find in module bs4.element:\n",
      "\n",
      "find(name=None, attrs={}, recursive=True, text=None, **kwargs) method of bs4.BeautifulSoup instance\n",
      "    Return only the first child of this Tag matching the given\n",
      "    criteria.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(soup.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.freblogg.com/2018/01/webapp-with-flask-1.html\">Build A Web Application With Flask In Python Part I</a>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2', {'class':'post-title entry-title'}).find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Using `find()` like this in series, we can drill down a nested tag and fetch the innermost values as needed.\n",
    "\n",
    "To get the link of an `<a>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Build A Web Application With Flask In Python Part I'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor = soup.find('h2', {'class':'post-title entry-title'}).find('a')\n",
    "#help(anchor)\n",
    "#anchor.get_text()\n",
    "#anchor.getText()\n",
    "anchor.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.freblogg.com/2018/01/webapp-with-flask-1.html'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to get all the titles of the articles. Each element in the headers list is a tag something like this in simple form.\n",
    "\n",
    "`<h2><a>How to recover from 'git reset --hard\" | Git</a></h2>`\n",
    "\n",
    "The title even though it is under `<a>`, is technically also under `<h2>` as well. Which means we can just use `.text` on that and get the title.\n",
    "\n",
    "Finally we add this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Build A Web Application With Flask In Python Part I',\n",
       " 'Json Parsing With Python',\n",
       " 'Datasets In Apache Spark - Part 3 | Writing Datasets to Disk',\n",
       " 'Remove Duplicate Elements From An Array',\n",
       " 'Reduce Image Size With Python And Tinypng',\n",
       " 'Datasets In Apache Spark | Part 2',\n",
       " 'My Almost Fully Automated Blogging Workflow']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = list(map(lambda h: h.text.strip(), headers))\n",
    "#titles = list(map(lambda h: h.getText(), headers))\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Build A Web Application With Flask In Python P...</td>\n",
       "      <td>http://www.freblogg.com/2018/01/webapp-with-fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Json Parsing With Python</td>\n",
       "      <td>http://www.freblogg.com/2018/01/json-parsing-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datasets In Apache Spark - Part 3 | Writing Da...</td>\n",
       "      <td>http://www.freblogg.com/2018/01/apache-spark-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remove Duplicate Elements From An Array</td>\n",
       "      <td>http://www.freblogg.com/2018/01/remove-duplica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reduce Image Size With Python And Tinypng</td>\n",
       "      <td>http://www.freblogg.com/2018/01/resize-compres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Datasets In Apache Spark | Part 2</td>\n",
       "      <td>http://www.freblogg.com/2018/01/apache-spark-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My Almost Fully Automated Blogging Workflow</td>\n",
       "      <td>http://www.freblogg.com/2017/12/my-automated-b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Build A Web Application With Flask In Python P...   \n",
       "1                           Json Parsing With Python   \n",
       "2  Datasets In Apache Spark - Part 3 | Writing Da...   \n",
       "3            Remove Duplicate Elements From An Array   \n",
       "4          Reduce Image Size With Python And Tinypng   \n",
       "5                  Datasets In Apache Spark | Part 2   \n",
       "6        My Almost Fully Automated Blogging Workflow   \n",
       "\n",
       "                                                link  \n",
       "0  http://www.freblogg.com/2018/01/webapp-with-fl...  \n",
       "1  http://www.freblogg.com/2018/01/json-parsing-w...  \n",
       "2  http://www.freblogg.com/2018/01/apache-spark-d...  \n",
       "3  http://www.freblogg.com/2018/01/remove-duplica...  \n",
       "4  http://www.freblogg.com/2018/01/resize-compres...  \n",
       "5  http://www.freblogg.com/2018/01/apache-spark-d...  \n",
       "6  http://www.freblogg.com/2017/12/my-automated-b...  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_links = dict(map(lambda h: (h.text.strip(),h.find('a')['href']), headers))\n",
    "pd.DataFrame(list(titles_links.items()), columns=['title', 'link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: [Web Scraping Tutorial with Python: Tips and Tricks](https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071)\n",
    "* [What are the differences between the urllib, urllib2, and requests module?\n",
    "](https://stackoverflow.com/questions/2018026/what-are-the-differences-between-the-urllib-urllib2-and-requests-module)\n",
    "\n",
    "There is a stand-alone ready-to-use data extracting framework called Scrapy. Apart from extracting HTML the package offers lots of functionalities like exporting data in formats, logging etc. It is also highly customisable: run different spiders on different processes, disable cookies¹ and set download delays². It can also be used to extract data using API. However, the learning curve is not smooth for the new programmers: you need to read tutorials and examples to get started.\n",
    "\n",
    "For my use case it was too much ‘out of the box’: I just wanted to extract the links from all pages, access each link and extract information out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.malaysiakini.com/en/latest/news'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.malaysiakini.com/en/latest/news'\n",
    "r = requests.get(url)\n",
    "r.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing Parameters In URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.malaysiakini.com/en/latest/news?key1=value1&key2=value2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.get(url, params=payload)\n",
    "r.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Content\n",
    "We can read the content of the server’s response. Consider the GitHub timeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Response in module requests.models object:\n",
      "\n",
      "class Response(builtins.object)\n",
      " |  The :class:`Response <Response>` object, which contains a\n",
      " |  server's response to an HTTP request.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      Returns True if :attr:`status_code` is less than 400.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code, is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Allows you to use a response as an iterator.\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |      Returns True if :attr:`status_code` is less than 400.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code, is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  close(self)\n",
      " |      Releases the connection back to the pool. Once this method has been\n",
      " |      called the underlying ``raw`` object must not be accessed again.\n",
      " |      \n",
      " |      *Note: Should not normally need to be called explicitly.*\n",
      " |  \n",
      " |  iter_content(self, chunk_size=1, decode_unicode=False)\n",
      " |      Iterates over the response data.  When stream=True is set on the\n",
      " |      request, this avoids reading the content at once into memory for\n",
      " |      large responses.  The chunk size is the number of bytes it should\n",
      " |      read into memory.  This is not necessarily the length of each item\n",
      " |      returned as decoding can take place.\n",
      " |      \n",
      " |      chunk_size must be of type int or None. A value of None will\n",
      " |      function differently depending on the value of `stream`.\n",
      " |      stream=True will read data as it arrives in whatever size the\n",
      " |      chunks are received. If stream=False, data is returned as\n",
      " |      a single chunk.\n",
      " |      \n",
      " |      If decode_unicode is True, content will be decoded using the best\n",
      " |      available encoding based on the response.\n",
      " |  \n",
      " |  iter_lines(self, chunk_size=512, decode_unicode=None, delimiter=None)\n",
      " |      Iterates over the response data, one line at a time.  When\n",
      " |      stream=True is set on the request, this avoids reading the\n",
      " |      content at once into memory for large responses.\n",
      " |      \n",
      " |      .. note:: This method is not reentrant safe.\n",
      " |  \n",
      " |  json(self, **kwargs)\n",
      " |      Returns the json-encoded content of a response, if any.\n",
      " |      \n",
      " |      :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n",
      " |      :raises ValueError: If the response body does not contain valid json.\n",
      " |  \n",
      " |  raise_for_status(self)\n",
      " |      Raises stored :class:`HTTPError`, if one occurred.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  apparent_encoding\n",
      " |      The apparent encoding, provided by the chardet library.\n",
      " |  \n",
      " |  content\n",
      " |      Content of the response, in bytes.\n",
      " |  \n",
      " |  is_permanent_redirect\n",
      " |      True if this Response one of the permanent versions of redirect.\n",
      " |  \n",
      " |  is_redirect\n",
      " |      True if this Response is a well-formed HTTP redirect that could have\n",
      " |      been processed automatically (by :meth:`Session.resolve_redirects`).\n",
      " |  \n",
      " |  links\n",
      " |      Returns the parsed header links of the response, if any.\n",
      " |  \n",
      " |  next\n",
      " |      Returns a PreparedRequest for the next request in a redirect chain, if there is one.\n",
      " |  \n",
      " |  ok\n",
      " |      Returns True if :attr:`status_code` is less than 400.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code, is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  text\n",
      " |      Content of the response, in unicode.\n",
      " |      \n",
      " |      If Response.encoding is None, encoding will be guessed using\n",
      " |      ``chardet``.\n",
      " |      \n",
      " |      The encoding of the response content is determined based solely on HTTP\n",
      " |      headers, following RFC 2616 to the letter. If you can take advantage of\n",
      " |      non-HTTP knowledge to make a better guess at the encoding, you should\n",
      " |      set ``r.encoding`` appropriately before accessing this property.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __attrs__ = ['_content', 'status_code', 'headers', 'url', 'history', '...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(url)\n",
    "help(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://api.github.com/events')\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "STBusiness_link ='https://www.straitstimes.com/business'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time-out**: You can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely.\n",
    "\n",
    "Timeout is not a time limit on the entire response download; rather, an exception is raised if the server has not issued a response for timeout seconds (more precisely, if no bytes have been received on the underlying socket for timeout seconds). If no timeout is specified explicitly, requests do not time out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the content from url\n",
    "page_response = requests.get(STBusiness_link)\n",
    "# parse html\n",
    "page_content = BS(page_response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all html elements where price is stored\n",
    "#prices = page_content.find_all(class_='block-link')\n",
    "divs = page_content.find_all('div', attrs = {'class':'view-content'})\n",
    "anchor = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.[A web scraper to retrieve stock indices automatically](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe)\n",
    "\n",
    "* [Scraping javascript page with PyQt5 and QWebEngineView\n",
    "](https://stackoverflow.com/questions/45265143/scraping-javascript-page-with-pyqt5-and-qwebengineview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tags\n",
    "\n",
    "```HTML\n",
    "<!DOCTYPE html>  \n",
    "<html>  \n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1> First Scraping </h1>\n",
    "        <p> Hello World </p>\n",
    "    <body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "This is the basic syntax of an HTML webpage. Every `<tag>` serves a block inside the webpage:\n",
    "1. `<!DOCTYPE html>`: HTML documents must start with a type declaration.\n",
    "2. The HTML document is contained between `<html>` and `</html>`.\n",
    "3. The meta and script declaration of the HTML document is between `<head>` and `</head>`.\n",
    "4. The visible part of the HTML document is between `<body>` and `</body>` tags.\n",
    "5. Title headings are defined with the `<h1>` through `<h6>` tags.\n",
    "6. Paragraphs are defined with the `<p>` tag.\n",
    "\n",
    "Other useful tags include `<a>` for hyperlinks, `<table>` for tables, `<tr>` for table rows, and `<td>` for table columns.\n",
    "\n",
    "Also, HTML tags sometimes come with id or class attributes. The id attribute specifies a unique id for an HTML tag and the value must be unique within the HTML document. The class attribute is used to define equal styles for HTML tags with the same class. We can make use of these ids and classes to help us locate the data we want.\n",
    "\n",
    "**Scraping Rules**\n",
    "\n",
    "* You should check a website’s Terms and Conditions before you scrape it. Be careful to read the statements about legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "* Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "* The layout of a website may change from time to time, so make sure to revisit the site and rewrite your code as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication\n",
    "from PyQt5.QtCore import QUrl\n",
    "from PyQt5.QtWebEngineWidgets import QWebEngineView\n",
    "\n",
    "class Render(QWebEngineView):\n",
    "    def __init__(self, url):\n",
    "        self.html = None\n",
    "        \n",
    "        if not QApplication.instance():\n",
    "            self.app = QApplication(sys.argv)\n",
    "        else:\n",
    "            self.app = QApplication.instance() \n",
    "\n",
    "        #self.app = QApplication(sys.argv)\n",
    "        \n",
    "        QWebEngineView.__init__(self)\n",
    "        self.loadFinished.connect(self._loadFinished)\n",
    "        #self.setHtml(html)\n",
    "        self.load(QUrl(url))\n",
    "        self.app.exec_()\n",
    "\n",
    "    def _loadFinished(self, result):\n",
    "        # This is an async call, you need to wait for this\n",
    "        # to be called before closing the app\n",
    "        self.page().toHtml(self._callable)\n",
    "\n",
    "    def _callable(self, data):\n",
    "        self.html = data\n",
    "        # Data has been stored, it's safe to quit the app\n",
    "        self.app.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Look at you shinin!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "from lxml import html\n",
    "url = 'https://pythonprogramming.net/parsememcparseface/' \n",
    "client_source = Render(url).html\n",
    "\n",
    "soup = bs.BeautifulSoup(client_source, 'lxml')\n",
    "js_test = soup.find('p', attrs = {'class': 'jstest'})\n",
    "js_test.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "page_link = 'https://www.bloomberg.com/quote/SPX:IND'\n",
    "source = Render(page_link).html\n",
    "\n",
    "page_content = bs.BeautifulSoup(source, \"html.parser\")\n",
    "\n",
    "div = page_content.find_all('div', \n",
    "                            attrs = {'class':'overviewRow__0956421f'})\n",
    "div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "r = {'is_claimed': 'True', 'rating': 3.5}\n",
    "r = json.dumps(r)\n",
    "loaded_r = json.loads(r)\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_r['rating'] #Output 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r) #Output str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded_r) #Output dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Newspaper3k: Article scraping & curation](https://newspaper.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limit for number of articles to download\n",
    "LIMIT = 50\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNewsPapers = {\"malaysiakini\": {\"link\": \"https://www.malaysiakini.com/en/latest/news\"},\\n              \"bbc\": {\"rss\": \"http://feeds.bbci.co.uk/news/rss.xml\", \"link\": \"http://www.bbc.com/\"}}\\n\\nwith open(\\'NewsPapers.json\\', \\'w\\') as in_file:\\n    companies = json.dump(NewsPapers, in_file)\\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NewsPapers = {\"malaysiakini\": {\"link\": \"https://www.malaysiakini.com/en/latest/news\"},\n",
    "              \"bbc\": {\"rss\": \"http://feeds.bbci.co.uk/news/rss.xml\", \"link\": \"http://www.bbc.com/\"}}\n",
    "\n",
    "with open('NewsPapers.json', 'w') as in_file:\n",
    "    companies = json.dump(NewsPapers, in_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'malaysiakini': {'link': 'https://www.malaysiakini.com/en/latest/news'},\n",
       " 'bbc': {'rss': 'http://feeds.bbci.co.uk/news/rss.xml',\n",
       "  'link': 'http://www.bbc.com/'}}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the JSON files with news sites\n",
    "with open('NewsPapers.json') as data_file:\n",
    "    companies = json.load(data_file)\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building site for  malaysiakini\n",
      "118\n",
      "36  Article has date of type None...\n",
      "37 https://www.malaysiakini.com/news/439936 2018-08-22 12:38:00+08:00 Guan Eng r\n",
      "38 https://www.malaysiakini.com/news/439931 2018-08-22 12:31:00+08:00 MCA must c\n",
      "39 https://www.malaysiakini.com/news/439926 2018-08-22 10:16:00+08:00 Keeping Ha\n",
      "40 https://www.malaysiakini.com/news/439922 2018-08-22 09:23:00+08:00 Top 5 back\n",
      "41 https://www.malaysiakini.com/news/439928 2018-08-22 11:00:00+08:00 Wan Azizah\n",
      "42 https://www.malaysiakini.com/news/439929 2018-08-22 11:19:00+08:00 Report: Pr\n",
      "43 https://www.malaysiakini.com/news/439918 2018-08-22 07:54:00+08:00 Seri Setia\n",
      "44 https://www.malaysiakini.com/news/439913 2018-08-22 07:01:00+08:00 New rules \n",
      "45 https://www.malaysiakini.com/news/439916 2018-08-22 07:38:00+08:00 Yoursay: D\n",
      "46 https://www.malaysiakini.com/news/439915 2018-08-22 07:22:00+08:00 Three Chin\n",
      "47 https://www.malaysiakini.com/news/439914 2018-08-22 07:16:00+08:00 Selamat Ha\n",
      "48 https://www.malaysiakini.com/news/439904 2018-08-21 23:31:00+08:00 Wee: Mahat\n",
      "49 https://www.malaysiakini.com/news/439894 2018-08-21 21:56:00+08:00 Musa Aman \n",
      "50 https://www.malaysiakini.com/news/439891 2018-08-21 21:05:00+08:00 MEIO gets \n",
      "51 https://www.malaysiakini.com/news/439879 2018-08-21 18:30:00+08:00 F-word to \n",
      "52 https://www.malaysiakini.com/news/439880 2018-08-21 18:50:00+08:00 Meet the g\n",
      "53 https://www.malaysiakini.com/news/439865 2018-08-21 17:44:00+08:00 'Walking t\n",
      "54 https://www.malaysiakini.com/news/439846 2018-08-21 15:30:00+08:00 PM rues Na\n",
      "55 https://www.malaysiakini.com/news/439772 2018-08-21 06:28:00+08:00 Yoursay: M\n",
      "56 https://www.malaysiakini.com/news/439885 2018-08-21 19:06:00+08:00 After ‘mis\n",
      "57 https://www.malaysiakini.com/news/439874 2018-08-21 18:11:00+08:00 You could \n",
      "58 https://www.malaysiakini.com/news/439789 2018-08-21 09:48:00+08:00 Musa to re\n",
      "59 https://www.malaysiakini.com/news/439856 2018-08-21 16:35:00+08:00 Cops recor\n",
      "60 https://www.malaysiakini.com/news/439771 2018-08-21 06:02:00+08:00 CEP 100-da\n",
      "You must `download()` an article first!\n",
      "\n",
      "continuing...\n",
      "61 https://www.malaysiakini.com/news/439807 2018-08-21 12:01:00+08:00 Make CEP's\n",
      "62 https://www.malaysiakini.com/news/439889 2018-08-21 21:00:00+08:00 PKR Youth:\n",
      "63 https://www.malaysiakini.com/news/439773 2018-08-21 06:38:00+08:00 Yoursay: P\n",
      "64 https://www.malaysiakini.com/news/439852 2018-08-21 16:22:00+08:00 100 days –\n",
      "65 https://www.malaysiakini.com/news/439822 2018-08-21 13:10:00+08:00 'Slower ec\n",
      "66 https://www.malaysiakini.com/news/439827 2018-08-21 13:33:00+08:00 I want him\n",
      "You must `download()` an article first!\n",
      "\n",
      "continuing...\n",
      "67 https://www.malaysiakini.com/news/439858 2018-08-21 16:51:00+08:00 Bull zips \n",
      "68 https://www.malaysiakini.com/news/439854 2018-08-21 16:24:00+08:00 Residents \n",
      "69 https://www.malaysiakini.com/news/439841 2018-08-21 14:55:00+08:00 Syed Saddi\n",
      "70 https://www.malaysiakini.com/news/439832 2018-08-21 13:57:00+08:00 'You have \n",
      "71 https://www.malaysiakini.com/news/439888 2018-08-21 20:15:00+08:00 Khalid Sam\n",
      "72 https://www.malaysiakini.com/news/439823 2018-08-21 13:25:00+08:00 Dr M: I'll\n",
      "73 https://www.malaysiakini.com/news/439811 2018-08-21 12:20:00+08:00 PKR MP apo\n",
      "74 https://www.malaysiakini.com/news/439804 2018-08-21 11:53:00+08:00 Taib's dau\n",
      "75 https://www.malaysiakini.com/news/439871 2018-08-21 18:08:00+08:00 MACC raids\n",
      "76 https://www.malaysiakini.com/news/439833 2018-08-21 13:58:00+08:00 Halimey to\n",
      "77 https://www.malaysiakini.com/news/439840 2018-08-21 14:48:00+08:00 Pandan Jay\n",
      "78 https://www.malaysiakini.com/news/439830 2018-08-21 13:48:00+08:00 Mahathir s\n",
      "79 https://www.malaysiakini.com/news/439790 2018-08-21 10:13:00+08:00 M'sia, Chi\n",
      "80 https://www.malaysiakini.com/news/439774 2018-08-21 06:53:00+08:00 EAIC urged\n",
      "81 https://www.malaysiakini.com/news/439887 2018-08-21 20:07:00+08:00 Suhakam wa\n",
      "82 https://www.malaysiakini.com/news/439892 2018-08-21 21:30:00+08:00 RMAF: Publ\n",
      "83 https://www.malaysiakini.com/news/439868 2018-08-21 17:51:00+08:00 Merdeka pa\n",
      "84 https://www.malaysiakini.com/news/439908 2018-08-21 23:45:00+08:00 High seas \n",
      "85 https://www.malaysiakini.com/news/439786 2018-08-21 09:23:00+08:00 Thai court\n",
      "86 https://www.malaysiakini.com/news/439881 2018-08-21 18:51:00+08:00 Tsipras de\n",
      "87 https://www.malaysiakini.com/news/439911 2018-08-22 00:08:00+08:00 It was a m\n",
      "88 https://www.malaysiakini.com/news/439909 2018-08-21 23:59:00+08:00 Mortar fro\n",
      "89 https://www.malaysiakini.com/news/439806 2018-08-21 11:53:00+08:00 Australian\n",
      "90 https://www.malaysiakini.com/news/439779 2018-08-21 08:19:00+08:00 Trump says\n",
      "91 https://www.malaysiakini.com/news/439784 2018-08-21 08:51:00+08:00 Beijing to\n",
      "92 https://www.malaysiakini.com/news/439780 2018-08-21 08:30:00+08:00 As food cr\n",
      "93 https://www.malaysiakini.com/news/409777 2018-01-24 07:30:00+08:00 Why does M\n",
      "94  Article has date of type None...\n",
      "95 https://www.malaysiakini.com/news/439932 2018-08-22 12:33:00+08:00 Akar umbi \n",
      "96 https://www.malaysiakini.com/news/439934 2018-08-22 12:44:00+08:00 Pemilik EC\n",
      "97 https://www.malaysiakini.com/news/439927 2018-08-22 10:51:00+08:00 Wan Azizah\n",
      "98 https://www.malaysiakini.com/news/439924 2018-08-22 10:15:00+08:00 Dalil k'ja\n",
      "99 https://www.malaysiakini.com/news/439917 2018-08-22 08:04:00+08:00 Salam Aidi\n",
      "100 https://www.malaysiakini.com/news/439923 2018-08-22 09:37:00+08:00 Lawatan Ch\n",
      "101 https://www.malaysiakini.com/news/439919 2018-08-22 08:23:00+08:00 PKR perket\n",
      "102 https://www.malaysiakini.com/news/439925 2018-08-22 10:33:00+08:00 Aidiladha \n",
      "103 https://www.malaysiakini.com/news/439897 2018-08-21 22:37:00+08:00 Musa Aman \n",
      "104 https://www.malaysiakini.com/news/439890 2018-08-21 21:13:00+08:00 Kenali gad\n",
      "105 https://www.malaysiakini.com/news/439895 2018-08-21 22:15:00+08:00 MEIO dapat\n",
      "106 https://www.malaysiakini.com/news/439912 2018-08-22 00:37:00+08:00 MCA: Lawat\n",
      "107 https://www.malaysiakini.com/news/439893 2018-08-21 21:34:00+08:00 Cakap seru\n",
      "108 https://www.malaysiakini.com/news/439901 2018-08-21 23:13:00+08:00 PM syukur \n",
      "109 https://www.malaysiakini.com/news/439859 2018-08-21 20:46:00+08:00 Gila-gila \n",
      "110 https://www.malaysiakini.com/news/439855 2018-08-21 16:31:00+08:00 PM: Rundin\n",
      "111 https://www.malaysiakini.com/news/439902 2018-08-21 23:25:00+08:00 Thailand p\n",
      "112 https://www.malaysiakini.com/news/439900 2018-08-21 23:05:00+08:00 HFMD: 53,1\n",
      "113 https://www.malaysiakini.com/news/439848 2018-08-21 15:57:00+08:00 Polis ambi\n",
      "114 https://www.malaysiakini.com/news/439872 2018-08-21 18:10:00+08:00 Tajuddin: \n",
      "115 https://www.malaysiakini.com/news/439899 2018-08-21 22:47:00+08:00 Trafik per\n",
      "116 https://www.malaysiakini.com/news/439870 2018-08-21 18:03:00+08:00 Dr Halimah\n",
      "117 https://www.malaysiakini.com/news/439845 2018-08-21 15:37:00+08:00 3 bulan ti\n",
      "118 https://www.malaysiakini.com/news/439835 2018-08-21 14:26:00+08:00 'Biarlah s\n",
      "119 https://www.malaysiakini.com/news/439861 2018-08-21 17:23:00+08:00 Bomba keja\n",
      "120 https://www.malaysiakini.com/news/439910 2018-08-22 00:02:00+08:00 Pertempura\n",
      "121 https://www.malaysiakini.com/news/439907 2018-08-21 23:51:00+08:00 Bom Ops Da\n",
      "122 https://www.malaysiakini.com/news/439849 2018-08-21 15:56:00+08:00 K’jaan sas\n",
      "123  Article has date of type None...\n",
      "124 https://www.malaysiakini.com/news/439930 2018-08-22 12:02:00+08:00 禁办大集会晒合照，公\n",
      "125 https://www.malaysiakini.com/news/439921 2018-08-22 09:15:00+08:00 “没解决不了的问题”\n",
      "126 https://www.malaysiakini.com/news/439905 2018-08-21 23:38:00+08:00 苏伯利填补哈萨那空缺\n",
      "127 https://www.malaysiakini.com/news/439896 2018-08-21 22:32:00+08:00 “有问题应友好协商解\n",
      "128 https://www.malaysiakini.com/news/439898 2018-08-21 22:40:00+08:00 慕沙阿曼傍晚抵马，移\n",
      "129 https://www.malaysiakini.com/news/439867 2018-08-21 19:11:00+08:00 从“X你”到“地狱”\n",
      "130 https://www.malaysiakini.com/news/439886 2018-08-21 19:33:00+08:00 与副手沟通失误，祖莱\n",
      "131 https://www.malaysiakini.com/news/439869 2018-08-21 17:58:00+08:00 当场退回获赠手机，陆\n",
      "132 https://www.malaysiakini.com/news/439866 2018-08-21 17:46:00+08:00 视政府财力决定，副财\n",
      "133 https://www.malaysiakini.com/news/439864 2018-08-21 17:44:00+08:00 查消费税退款消失，警\n",
      "134 https://www.malaysiakini.com/news/439863 2018-08-21 17:28:00+08:00 查沙森林局贪污案，反\n",
      "135 https://www.malaysiakini.com/news/439860 2018-08-21 17:17:00+08:00 政府致力长期财政整顿\n",
      "136 https://www.malaysiakini.com/news/439853 2018-08-21 16:29:00+08:00 “前所未见的愚蠢”，\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 https://www.malaysiakini.com/news/439851 2018-08-21 16:04:00+08:00 “刘特佐当年权势才大\n",
      "138 https://www.malaysiakini.com/news/439837 2018-08-21 14:36:00+08:00 已获中国谅解不反对，\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/song/.netrc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-710aadac253a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, input_html, title, recursion_counter)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_html\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_html_2XX_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/newspaper/network.py\u001b[0m in \u001b[0;36mget_html_2XX_only\u001b[0;34m(url, config, response)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         response = requests.get(\n\u001b[0;32m---> 64\u001b[0;31m             url=url, **get_request_kwargs(timeout, useragent, proxies, headers))\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get_html_2XX_only() error. %s on URL: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         )\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrust_env\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_netrc_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/utils.py\u001b[0m in \u001b[0;36mget_netrc_auth\u001b[0;34m(url, raise_errors)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0mnetrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through each news company\n",
    "for company, value in companies.items():\n",
    "    if 'rss' in value:\n",
    "        rss = value['rss']\n",
    "        d = fp.parse(rss)\n",
    "        print('###############################################################')\n",
    "        print(rss)\n",
    "        #print(d.entries[0].keys())\n",
    "        print(\"Downloading articles from \", company)\n",
    "        newsPaper = {\n",
    "            \"rss\": value['rss'],\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        \n",
    "        count = 1\n",
    "        \n",
    "        #every entry is a news\n",
    "        for entry in d.entries:\n",
    "            #print(entry.keys())\n",
    "            if hasattr(entry, 'published'):\n",
    "            #if 'published' in entry.keys():\n",
    "                if count > LIMIT:\n",
    "                    break\n",
    "                article = {}\n",
    "                article['link'] = entry.link\n",
    "                date = entry.published_parsed\n",
    "                #print(entry.link)\n",
    "                #print(date)\n",
    "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
    "                \n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"continuing...\")\n",
    "                    continue\n",
    "                  \n",
    "                article['title'] = content.title\n",
    "                article['text'] = content.text[:10]\n",
    "                print(article)\n",
    "\n",
    "                \n",
    "                newsPaper['articles'].append(article)\n",
    "                #print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
    "                count = count + 1        \n",
    "\n",
    "    else:\n",
    "        # This is the fallback method if a RSS-feed link is not provided.\n",
    "        # It uses the python newspaper library to extract articles\n",
    "        print(\"Building site for \", company)\n",
    "        paper = newspaper.build(value['link'], memoize_articles=False, language='en')\n",
    "        #print(help(paper))\n",
    "        print(len(paper.articles))\n",
    "        \n",
    "        newsPaper = {\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        noneTypeCount = 0\n",
    "        \n",
    "        #each content is an article\n",
    "        for content in paper.articles:\n",
    "            #if count > LIMIT:\n",
    "             #   break\n",
    "            if content.title is 'en':\n",
    "                continue\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "                continue\n",
    "                \n",
    "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "            if content.publish_date is None:\n",
    "                print(count, \" Article has date of type None...\")\n",
    "                noneTypeCount = noneTypeCount + 1\n",
    "                if noneTypeCount > 10:\n",
    "                    print(\"Too many noneType dates, aborting...\")\n",
    "                    noneTypeCount = 0\n",
    "                    break\n",
    "                count = count + 1\n",
    "                continue\n",
    "                \n",
    "            article = {}\n",
    "            article['title'] = content.title\n",
    "            article['text'] = content.text\n",
    "            article['link'] = content.url\n",
    "            article['published'] = content.publish_date.isoformat()\n",
    "            \n",
    "            newsPaper['articles'].append(article)\n",
    "            #print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
    "            print(count, content.url, content.publish_date, content.title[:10])\n",
    "            count = count + 1\n",
    "            noneTypeCount = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Check robots.txt**\n",
    "\n",
    "The scraping rules of the websites can be found in the `robots.txt` file. You can find it by writing robots.txt after the main domain, e.g `www.website_to_scrape.com/robots.txt`. These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page. Most people don’t care about it, but try to be respectful and at least look at the rules even if you don’t plan to follow them.\n",
    "\n",
    "**3.2 HTML can be evil**\n",
    "\n",
    "HTML tags can contain id, class or both. HTML id specifies a unique id and HTML class is non-unique. Changes in the class name or element could either break your code or deliver wrong results.\n",
    "\n",
    "There are two ways to avoid it or at least to be alerted about it:\n",
    "\n",
    "* Use specific id rather than class since it is less likely to be changed\n",
    "* Check if the element returns None\n",
    "\n",
    "```python\n",
    "price = page_content.find(id='listings_prices')\n",
    "# check if the element with such id exists or not\n",
    "if price is None:\n",
    "    # NOTIFY! LOG IT, COUNT IT\n",
    "else:\n",
    "    # do something\n",
    "```\n",
    "\n",
    "However, because some fields can be optional (like `discounted_price` in our HTML example), corresponding elements would not appear on each listing. In this case you can count the percentage of how many times this specific element returned `None` to the number of listings. If it is 100%, you might want to check if the element name was changed.\n",
    "\n",
    "**3.3 User agent spoofing**\n",
    "\n",
    "Everytime you visit a website, it gets your browser information via user agent. Some websites won’t show you any content unless you provide a user agent. Also, some sites offer different content to different browsers. Websites do not want to block genuine users but you would look suspicious if you send 200 requests/second with the same user agent. A way out might be either to generate (almost) random user agent or to set one yourself.\n",
    "\n",
    "```python\n",
    "# library to generate user agent\n",
    "from user_agent import generate_user_agent\n",
    "# generate a user agent\n",
    "headers = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}\n",
    "#headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.63 Safari/537.36'}\n",
    "page_response = requests.get(page_link, timeout=5, headers=headers)\n",
    "```\n",
    "\n",
    "**3.4 Timeout request**\n",
    "\n",
    "By default, Request will keep waiting for a response indefinitely. Therefore, it is advised to set the timeout parameter.\n",
    "```python\n",
    "# timeout is set to 5 secodns\n",
    "page_response = requests.get(page_link, timeout=5, headers=headers)\n",
    "```\n",
    "\n",
    "**3.5 Did I get blocked?**\n",
    "\n",
    "Frequent appearance of the status codes like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked. You may want to check for those error codes and proceed accordingly.\n",
    "Also, be ready to handle exceptions from the request.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    if page_response.status_code == 200:\n",
    "        # extract\n",
    "    else:\n",
    "        print(page_response.status_code)\n",
    "        # notify, try again\n",
    "except requests.Timeout as e:\n",
    "    print(\"It is time to timeout\")\n",
    "    print(str(e))\n",
    "except # other exception\n",
    "```\n",
    "\n",
    "**3.6 IP Rotation**\n",
    "\n",
    "Even if you randomize your user agent, all your requests will be from the same IP address. That doesn’t sound abnormal because libraries, universities, and also companies have only a few IP addresses. However, if there are uncommonly many requests coming from a single IP address, a server can detect it. \n",
    "Using shared `proxies, VPNs or TOR` can help you become a ghost :).\n",
    "```python\n",
    "proxies = {'http' : 'http://10.10.0.0:0000',  \n",
    "          'https': 'http://120.10.0.0:0000'}\n",
    "page_response = requests.get(page_link, proxies=proxies, timeout=5)  \n",
    "\n",
    "```\n",
    "By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website.\n",
    "\n",
    "\n",
    "**3.7 Honeypots**\n",
    "\n",
    "Honeypots are means to detect crawlers or scrapers.\n",
    "\n",
    "These can be ‘hidden’ links that are not visible to the users but can be extracted by scrapers/spiders. Such links will have a CSS style set to `display:none`, they can be blended by having the color of the background, or even be moved off of the visible area of the page. Once your crawler visits such a link, your IP address can be flagged for further investigation, or even be instantly blocked.\n",
    "\n",
    "Another way to spot crawlers is to add links with infinitely deep directory trees. Then one would need to limit the number of retrieved pages or limit the traversal depth.\n",
    "\n",
    "\n",
    "**4. Dos and Don’ts**\n",
    "* Before scraping, check if there is a public API available. Public APIs provide easier and faster (and legal) data retrieval than web scraping. Check out Twitter API that provides APIs for different purposes.\n",
    "* In case you scrape lots of data, you might want to consider using a database to be able to analyze or retrieve it fast. Follow this tutorial on how to create a local database with python.\n",
    "* Be polite. As this answer suggests, it is recommended to let people know that you are scraping their website so they can better respond to the problems your bot might cause.\n",
    "Again, do not overload the website by sending hundreds of requests per second.\n",
    "\n",
    "\n",
    "**5. Speed up — parallelization**\n",
    "If you decide to parallelize your program, be careful with your implementation so you don’t slam the server. And be sure you read the Dos and Don’ts section. Check out the the definitions of parallelization vs concurrency, processors and threads here and here.\n",
    "\n",
    "If you extract a huge amount of information from the page and do some preprocessing of the data while scraping, the number of requests per second you send to the page can be relatively low.\n",
    "\n",
    "For my other project where I scraped apartment rental prices, I did heavy preprocessing of the data while scraping, which resulted in 1 request/second. In order to scrape 4K ads, my program would run for about one hour.\n",
    "\n",
    "In order to send requests in parallel you might want to use a multiprocessing package.\n",
    "\n",
    "Let’s say we have 100 pages and we want to assign every processor equal amount of pages to work with. If n is the number of CPUs, you can evenly chunk all pages into the n bins and assign each bin to a processor. Each process will have its own name, target function and the arguments to work with. The name of the process can be used afterwards to enable writing data to a specific file.\n",
    "\n",
    "I assigned 1K pages to each of my 4 CPUs which yielded 4 requests/second and reduced the scraping time to around 17 mins.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import multiprocessing as multi\n",
    "\n",
    "def chunks(n, page_list):\n",
    "    \"\"\"Splits the list into n chunks\"\"\"\n",
    "    return np.array_split(page_list,n)\n",
    " \n",
    "cpus = multi.cpu_count()\n",
    "workers = []\n",
    "page_list = ['www.website.com/page1.html', 'www.website.com/page2.html'\n",
    "             'www.website.com/page3.html', 'www.website.com/page4.html']\n",
    "\n",
    "page_bins = chunks(cpus, page_list)\n",
    "\n",
    "for cpu in range(cpus):\n",
    "    sys.stdout.write(\"CPU \" + str(cpu) + \"\\n\")\n",
    "    # Process that will send corresponding list of pages \n",
    "    # to the function perform_extraction\n",
    "    worker = multi.Process(name=str(cpu), \n",
    "                           target=perform_extraction, \n",
    "                           args=(page_bins[cpu],))\n",
    "    worker.start()\n",
    "    workers.append(worker)\n",
    "\n",
    "for worker in workers:\n",
    "    worker.join()\n",
    "    \n",
    "def perform_extraction(page_ranges):\n",
    "    \"\"\"Extracts data, does preprocessing, writes the data\"\"\"\n",
    "    # do requests and BeautifulSoup\n",
    "    # preprocess the data\n",
    "    file_name = multi.current_process().name+'.txt'\n",
    "    # write into current process file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
